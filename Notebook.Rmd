---
title: "R Notebook"
author: "Beatriz Estrella"
date: "26/10/2020"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 3)
options(tinytex.verbose = TRUE)
```

# 1. INTRODUCTION <a name="introduction"></a>
## 1.1. Project goal <a name="goal"></a>

The motivation of this project is to complete the task of the course in Data Science: Capstone from HarvardX, course PH125.9x, under the *Choose your own!* project submission.

Datasets, .R code and all relevant documentation regarding this project can be found online at [github.com/beatrizeg/Wish-Units-Solds](https://github.com/beatrizeg/Wish-Units-Solds).

```{r load Data and libraries, echo=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(purrr)) install.packages("purrr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(forcats)) install.packages("forcats", repos = "http://cran.us.r-project.org")
if(!require(rattle)) install.packages("rattle", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(klaR)) install.packages("klaR", repos = "http://cran.us.r-project.org")
if(!require(h2o)) install.packages("h2o", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(tictoc)) install.packages("tictoc", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(stringr)
library(purrr)
library(caret)
library(ggplot2)
library(corrplot)
library(forcats)
library(rattle)
library(xgboost)
library(klaR)
library(knitr)
library(tictoc)

#loading databases
url_train <- "https://raw.githubusercontent.com/beatrizeg/HousePrices/main/train.csv"
dest_file <- "./main.csv"
download.file(url_train, destfile = dest_file)
main <- read_csv("./main.csv")

url_test <- "https://raw.githubusercontent.com/beatrizeg/HousePrices/main/test.csv"
dest_file_tester <- "./tester.csv"
download.file(url_test, destfile = dest_file_tester)
test <- read_csv("./tester.csv")

main <- as.data.frame(main)
tester <- as.data.frame(test)
```

## 1.2. Inspecting the dataset <a name="inspect"></a>

Now we proceed to inspect the main dataset that is directly downloaded from Kaggle. Using the dim and summary functions we see:

```{r inspect}
dim(main)
summary(main)
```

The dataset consists of 1460 rows and 81 columns. We see that we have a mix of character and numeric values. 81 columns is quite a lot of columns as each of them would represent a feature and probably not all of them help predict the sale price of a house. We will try to reduce the number of columns early in the project to be able to work better with the data.

### 1.2.1. Dealing with NAs <a name="nas"></a>

First things first, let's check the NAs. To check which columns gives us NAs we use.

```{r nas}
nas <- apply(main, 2, function(x) any(is.na(x)))
knitr::kable(nas[which(nas)], caption = "Check columns with NAs")
```

We see that from a total of 81 columns, 19 of them contain NAs. If we check the data description file, let's describe each of these columns.

· LotFrontage: This is a numeric value that represents linear feet of street connected to property -> we have a total of *259/1460=17.7%* which can be acceptable to keep this data but NAs will be substituted by 0 so that we can perform calculations with this data.
· Alley: This is a character value in which NA represents that there is no alley access.
· MasVnrType: This is a character value in which NA is a *not applicable* value. Let's check how many of them we have.
```{r nas MasVnrType}
sum(is.na(main$MasVnrType))
```
We only have 8 values with NA so is does not seem like a reason to disregard the column data. 
· MasVnrArea: same as before, in this case it is a numeric value with same number of NAs.
· BsmtQual: it evaluates the height of the basement in a categorical value and we have 37 NAs.
· BsmtCond: evaluates the general condition of the basement and gives the same result as previous line.
· BsmtExposure, BsmtFinType1 and BsmtFinType2: gives NAs as well in case of no basement.
· Electrical: categorical feature with only 1 NA.
· FireplaceQu: it is a categorical value that provides the fireplace quality. NA represents no fireplace and in this case we get 690 values.
· Garage: all garage related features. They show different aspects of garages and NAs represent no garage.
· PoolQC: it shows pool quality and NA represents No Pool.
· Fence: represents fence quality and NA respresents No Fence.
· MiscFeature: it is a categorical miscellaneous feature such as elevator, tennis court, etc. NA represents None.

Let's clean all of these NAs in the dataset as per data description. We will leave the NAs in LotFrontage and Electrical because we do not know what they mean in the dataset.

```{r nas treat}
main <- main %>% mutate(LotFrontage=ifelse(is.na(LotFrontage),0,LotFrontage),
                        Alley=ifelse(is.na(Alley),'None',Alley),
                        Electrical=ifelse(is.na(Electrical),'Unknown',Electrical),
                        MasVnrType=ifelse(is.na(MasVnrType),'None',MasVnrType),
                        MasVnrArea=ifelse(is.na(MasVnrArea),0,MasVnrArea),
                        BsmtQual=ifelse(is.na(BsmtQual),'None',BsmtQual),
                        BsmtCond=ifelse(is.na(BsmtCond),'None',BsmtCond),
                        BsmtExposure=ifelse(is.na(BsmtExposure),'None',BsmtExposure),
                        BsmtFinType1=ifelse(is.na(BsmtFinType1),'None',BsmtFinType1),
                        BsmtFinType2=ifelse(is.na(BsmtFinType2),'None',BsmtFinType2),
                        FireplaceQu=ifelse(is.na(FireplaceQu),'None',FireplaceQu),
                        GarageType=ifelse(is.na(GarageType),'None',GarageType),
                        GarageYrBlt=ifelse(is.na(GarageYrBlt),'None',GarageYrBlt),
                        GarageFinish=ifelse(is.na(GarageFinish),'None',GarageFinish),
                        GarageQual=ifelse(is.na(GarageQual),'None',GarageQual),
                        GarageCond=ifelse(is.na(GarageCond),'None',GarageCond),
                        PoolQC=ifelse(is.na(PoolQC),'None',PoolQC),
                        Fence=ifelse(is.na(Fence),'None',Fence),
                        MiscFeature=ifelse(is.na(MiscFeature),'None',MiscFeature))
```

### 1.2.2. Dealing with factor variables <a name="character"></a>
Also and because we have an important number of character features, none of them being unique or with high variability, these variables should be converted to factor so they take less memory and faster our computation. Also, we will change numeric variables MSSubClass, OverallCond, OverallQual, YrSold and MoSold to factor as well. To do so, we can change all character variables to factor as one with:

```{r factor}
main[sapply(main, is.character)] <- lapply(main[sapply(main, is.character)], 
                                                           as.factor)

main$MSSubClass <- factor(main$MSSubClass, levels=c("20","30","40","45","50","60","70","75","80","85","90","120","150","160","180","190"))
main$OverallCond <- factor(main$OverallCond, levels=c("1","2","3","4","5","6","7","8","9","10"))
main$OverallQual <- factor(main$OverallQual, levels=c("1","2","3","4","5","6","7","8","9","10"))
main$YrSold <- factor(main$YrSold, levels=c("2006","2007","2008","2009","2010"))
main$MoSold <- factor(main$MoSold, levels=c("1","2","3","4","5","6","7","8","9","10","11","12"))
```

Lastly, it is important to note that the Id column provides a unique Id to each house.

# 2. METHOD AND ANALYSIS <a name="analysis"></a>
## 2.1. Exploration of the dataset <a name="exploration"></a>

Now we proceed to inspect the different features/variables in the dataset, taking into account that the value we want to be able to predict is *SalePrice*. 

### 2.1.1. Checking features variability and adjusting <a name="variability"></a>

Because checking 79 features one by one to analyze variability can take too long, let's start with analyzing those predictors that maintain close to same value across all the same to check if we want to disregard them.

```{r no_var}
no_var <- nearZeroVar(main, saveMetrics = TRUE)
knitr::kable(no_var[no_var[,"zeroVar"] + no_var[,"nzv"] > 0, ], 
             caption = "Predictors with near zero variability")
```

We do not get any feature with Zero Variance but we do get quite an important number of features with NearZeroVariance. Among all of them, we will eliminate from our model *Street*, *Alley*, *Condition2*, *RoofMatl*,*BsmtFinType2* and *BsmtFinSF2*, *LowQualFinSF* and *PoolQC*, as these are the variables with less variability and per the data description, we understand they have the less impact in a house sale price.

```{r select, echo=FALSE}
main_m <- main %>% dplyr::select(Id, MSSubClass, MSZoning, LotFrontage, LotArea, LotShape, LandContour, Utilities, LotConfig,
                                 LandSlope,Neighborhood,Condition1,BldgType,HouseStyle,OverallQual,OverallCond,YearBuilt,
                                 YearRemodAdd,RoofStyle,RoofMatl,Exterior1st,Exterior2nd,MasVnrType,MasVnrArea,ExterQual,ExterCond,
                                 Foundation,BsmtQual,
                                 BsmtCond,BsmtExposure,BsmtFinType1,BsmtFinSF1,BsmtFinType2,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,
                                 Heating,HeatingQC,CentralAir,Electrical,"1stFlrSF","2ndFlrSF",LowQualFinSF,GrLivArea,BsmtFullBath,
                                 BsmtHalfBath,FullBath,HalfBath,BedroomAbvGr,KitchenAbvGr,KitchenQual,TotRmsAbvGrd,Functional,
                                 Fireplaces,FireplaceQu,GarageType,GarageYrBlt,GarageFinish,GarageCars,GarageArea,GarageQual,
                                 GarageCond,PavedDrive,WoodDeckSF,OpenPorchSF,EnclosedPorch,"3SsnPorch",ScreenPorch,PoolArea,
                                 PoolQC,Fence,MiscFeature,MiscVal,MoSold,YrSold,SaleType,SaleCondition,SalePrice)
```

Now, from a start of 79 features, we continue with 71. 71 variables is still quite a lot.

## 2.2. Studying correlation between variables <a name="cor"></a>

Because we want to further study and understand if all 71 features should be kept in the model, we will now proceed with studying correlation among the different variables. For those that are high correlated to each other, only one of them should be kept in model disregarding the correlated ones.

As we have both numeric and string values, we need to study correlation separately. We will start first with the numeric values.

```{r cor, fig.height=6}
main_m.cor <- main_m %>%
  dplyr::select_if(is.numeric) %>%
  cor(.)
corrplot(main_m.cor, type="lower", tl.cex = 0.5)
```

When looking at the results we can see different interesting things. 

First of all, let's look at the SalePrice row, this is our dependent variable that we are trying to predict. We can see which features are most affecting the sale price of a house and which ones are independent. For example, the Overall Condition and the GrLivArea or Garage Car increases the Sale Price of a house. We can be tempted now to disregard the variables that seem to be independent of the Sale Price but, this is important, there are way better methods to do so and we can be disregarding some variables that affect other variables that highly affect the Sale Price, thus they can be important to keep.

What we want to do here is check the pair of variables that have a high correlation and remove one of them. For this, we can use the findCorrelation function from caret library. We see that the function recommends disregarding variable GrLivArea, which is highly correlated to TotRmsAbvGrd, also 1stFlrSF, which is highly correlated to TotalBsmtST and GarageCars, highly correlated to GarageArea.

```{r cor findcor}
highlyCorrelated <- caret::findCorrelation(main_m.cor, cutoff=0.7, names = TRUE)
```

We will remove those 3 variables from the ongoing model. Also, worth mentioning that of course, our SalePrice variable cannot be remove as this is the variable we are trying to predict.

To study correlation between non numerical variables, we will perform a Chi-squared test of independence, looking at the p-values. We can say that two different variables are independent if the probability distribution of one is not affected by the presence of the other.

In this case, the null hypotheses is that the variables are independent from the units sold. With a significance level of 0.05, we will test the hypotheses of them being independent.
```{r chisq dep}
main_m.chisq <- main_m %>%
  dplyr::select_if(function(col) is.character(col) | 
                     is.factor(col) | is.logical(col) |
                     all(col == .$SalePrice)) %>% dplyr::select(-.$Id)

columns <- 1:ncol(main_m.chisq)
vars <- names(main_m.chisq)[columns]
out <-  apply( combn(columns,2),2,function(x){
  chisq.test(table(main_m.chisq[,x[1]],main_m.chisq[,x[2]]),correct=F)$p.value
})

out <- cbind(as.data.frame(t(combn(vars,2))),out)
out_dep <- out %>% filter(V2=="SalePrice") %>% filter(out<0.05) %>% arrange(out)
knitr::kable(out_dep, caption = "Dependent categorical variables from SalePrice")
```

```{r chisq indep}
out_ind <- out %>% filter(V2=="SalePrice") %>% filter(out>=0.05) %>% arrange(out)
knitr::kable(out_ind, caption = "Independent categorical variables from SalePrice")
```

We can see that we get p-values below 0.05 for 19 variables, thus, we can reject in these cases the null hypothesis and assume that these are not independent to SalePrice. 

Also, we can assume that these 25 other predictors are independent and do not affect the output of the units sold. In fact, 14 of them have a p-value over 0.9 indicating that they are with a probability over 99% independent on the SalePrice. 

We will be able to check on this when we study the variable importance of the machine learning algorithms.

## 2.3. Checking predictors effect - graphs <a name="graphs"></a>

**ExterQual** = Quality of material on the exterior
As expected, the Excellent gives higher Sale Prices, then God, Average, Fair and Poor lastly.

```{r exterqual graph, echo=FALSE}
main_m %>% 
  ggplot(aes(fct_infreq(ExterQual), SalePrice)) + geom_boxplot() +
  ggtitle("ExterQual") + xlab("ExterQual")
```

**ExterQual** = Quality of material on the exterior
```{r Kitchenqual graph, echo=FALSE}
main_m %>% 
  ggplot(aes(fct_infreq(KitchenQual), SalePrice)) + geom_boxplot() +
  ggtitle("KitchenQual") + xlab("KitchenQual")
```

```{r other graph, echo=FALSE}

main_m %>% 
  ggplot(aes(fct_infreq(Heating), SalePrice)) + geom_boxplot() +
  ggtitle("Heating") + xlab("Heating")

main_m %>% 
  ggplot(aes(fct_infreq(Neighborhood), SalePrice)) + geom_boxplot() +
  ggtitle("Neighborhood") + xlab("Neighborhood")

#categorical independent
main_m %>% 
  ggplot(aes(fct_infreq(GarageYrBlt), SalePrice)) + geom_boxplot() +
  ggtitle("GarageYrBlt") + xlab("GarageYrBlt")

main_m %>% 
  ggplot(aes(fct_infreq(BldgType), SalePrice)) + geom_boxplot() +
  ggtitle("BldgType") + xlab("BldgType")

main_m %>% 
  ggplot(aes(fct_infreq(PoolQC), SalePrice)) + geom_boxplot() +
  ggtitle("PoolQC") + xlab("PoolQC")

main_m %>% 
  ggplot(aes(fct_infreq(MiscFeature), SalePrice)) + geom_boxplot() +
  ggtitle("MiscFeature") + xlab("MiscFeature")

main_m %>% 
  ggplot(aes(fct_infreq(OverallCond), SalePrice)) + geom_boxplot() +
  ggtitle("OverallCond") + xlab("OverallCond")

main_m %>% 
  ggplot(aes(SaleType, SalePrice)) + geom_jitter(aes(color=SaleCondition)) +
  ggtitle("SaleType") + xlab("SaleType")

main_m %>% 
  ggplot(aes(LotShape, SalePrice)) + geom_jitter(aes(color=LotConfig)) +
  ggtitle("LotShape") + xlab("LotShape")

#numerical
main_m %>% 
  ggplot(aes(GrLivArea, SalePrice)) + geom_smooth() +
  ggtitle("GrLivArea") + xlab("GrLivArea")

main_m %>% 
  ggplot(aes(GarageCars, SalePrice)) + geom_jitter(aes(color=BldgType)) +
  ggtitle("GarageCars") + xlab("GarageCars")

main_m %>% 
  ggplot(aes(TotalBsmtSF, SalePrice)) + geom_jitter(aes(color=BsmtCond)) +
  ggtitle("TotalBsmtSF") + xlab("TotalBsmtSF")

main_m %>% 
  ggplot(aes(YrSold, SalePrice)) + geom_jitter(aes(color=YearBuilt)) +
  ggtitle("YrSold") + xlab("YrSold")
```

## 2.4. Creation of train and test set <a name="sets"></a>

We split our data into a train set where we run the algorithms, and a test set to test the results and see how good our algorithm did. Test set usually consists of 10%-20% of the data. In our case, test set will have 15% of the data and train set 85%. As we do not have too much data, we do not want to compromise test set to have very few rows, or train set to not be big enough to produce good models.

Our train set consists of 1138 rows and test set of 203 rows.

```{r train test}
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(main_m$SalePrice, times=1, p=0.15, list=FALSE)
train_set <- main_m[-test_index,] %>% dplyr::select(-Id)
test_set <- main_m[test_index,] %>% dplyr::select(-Id) 
```

## 2.5. Method <a name="methods"></a>

The method will be to train different Machine Learning algorithms, that are included in `caret` package. We will train the models using only the train_set, while repeated cross-validation or simple cross-validation will be used to optimize the hyperparameters of each model. Once the model is optimized using only the train_set, results will be tested in test_set.

The methods to use are those appropriate to study a regression problem such as this one. Thus, we will be using *Linear Regression Model*, *Neural Network Models*, *Random Forest*, *SVM* or *Lasso Regression*.

As well, `h2o` package will be used at the end, to check which is the best performer model and compare results.

# 3. RESULTS <a name="results"></a>

## 3.1. GAM Loess <a name="gam"></a>